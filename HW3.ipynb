{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cd3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b408e3",
   "metadata": {},
   "source": [
    "## Part1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b627574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11903, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98207cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4938, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(['Summary', 'Text'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00b9b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B002QWP89S', 'B000KV61FC', 'B0013NUGDE', 'B001EO5Q64',\n",
       "       'B003B3OOPA', 'B0013A0QXC', 'B005K4Q1YA', 'B007JFMH8M'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProductId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec29f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ProductId', 'Summary', 'Text', 'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f4e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_all'] = df['Summary'] + ' ' + df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6025cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(words, stop=None):\n",
    "    '''Set up countvectorizer with several parameters, print shape of vectorizer and return vectorizer in DataFrame'''\n",
    "    vectorizer = CountVectorizer(stop_words=stop, lowercase=True, min_df=0.001) # only keep keywords that appear in more than 0.1% of the reviews\n",
    "    X = vectorizer.fit_transform(words) \n",
    "    X = X.toarray()\n",
    "    print(X.shape)\n",
    "    feature = vectorizer.get_feature_names()\n",
    "    corpus_df = pd.DataFrame(X, columns=feature)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b5a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4938, 3147)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "br         3821\n",
       "great      2443\n",
       "oil        2271\n",
       "good       2055\n",
       "coconut    1924\n",
       "love       1911\n",
       "like       1899\n",
       "coffee     1820\n",
       "product    1674\n",
       "just       1413\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = get_vec(df['text_all'].tolist(), 'english')\n",
    "corpus_df.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77db5e1",
   "metadata": {},
   "source": [
    "To choose 2-4 products as target for TF-IDF, I group reviews by product ID and observe the top 3 keywords for each product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4e3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_vec(df, stop=None):\n",
    "    pid = df.ProductId.unique().tolist()\n",
    "    overall = pd.DataFrame(columns=[1, 2, 3], index=pid)\n",
    "    for i in pid:\n",
    "        review = df[df['ProductId'] == i]['text_all'].tolist()\n",
    "        vectorizer = CountVectorizer(stop_words=stop, lowercase=True, min_df=0.001)\n",
    "        X = vectorizer.fit_transform(review) \n",
    "        X = X.toarray()\n",
    "        feature = vectorizer.get_feature_names()\n",
    "        good_df = pd.DataFrame(X, columns=feature)\n",
    "        keyword = good_df.sum().sort_values(ascending=False)[:3].index.tolist()\n",
    "        for j in range(3):\n",
    "            overall.loc[i, j+1] = keyword[j]\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88698a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B002QWP89S</th>\n",
       "      <td>greenies</td>\n",
       "      <td>dog</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B000KV61FC</th>\n",
       "      <td>toy</td>\n",
       "      <td>dog</td>\n",
       "      <td>br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0013NUGDE</th>\n",
       "      <td>chips</td>\n",
       "      <td>br</td>\n",
       "      <td>flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B001EO5Q64</th>\n",
       "      <td>oil</td>\n",
       "      <td>coconut</td>\n",
       "      <td>br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B003B3OOPA</th>\n",
       "      <td>oil</td>\n",
       "      <td>coconut</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0013A0QXC</th>\n",
       "      <td>coffee</td>\n",
       "      <td>senseo</td>\n",
       "      <td>pods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B005K4Q1YA</th>\n",
       "      <td>coffee</td>\n",
       "      <td>cappuccino</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B007JFMH8M</th>\n",
       "      <td>cookie</td>\n",
       "      <td>cookies</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1           2       3\n",
       "B002QWP89S  greenies         dog    dogs\n",
       "B000KV61FC       toy         dog      br\n",
       "B0013NUGDE     chips          br  flavor\n",
       "B001EO5Q64       oil     coconut      br\n",
       "B003B3OOPA       oil     coconut    hair\n",
       "B0013A0QXC    coffee      senseo    pods\n",
       "B005K4Q1YA    coffee  cappuccino    like\n",
       "B007JFMH8M    cookie     cookies    soft"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall = get_product_vec(df, 'english')\n",
    "overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b916c",
   "metadata": {},
   "source": [
    "I choose B001EO5Q64 and B003B3OOPA as target since they share similar keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d4eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked_pid = ['B001EO5Q64', 'B003B3OOPA']\n",
    "df = df[df['ProductId'].isin(picked_pid)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282874ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_review = df[df['Score'] >= 4]['text_all'].tolist()\n",
    "poor_review = df[df['Score'] < 4]['text_all'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba01c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124, 2929)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oil        2130\n",
       "coconut    1840\n",
       "br         1209\n",
       "use         961\n",
       "hair        864\n",
       "product     845\n",
       "great       845\n",
       "skin        662\n",
       "love        598\n",
       "good        536\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_review_vec = get_vec(good_review, 'english')\n",
    "good_review_vec.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789d41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 1205)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oil          97\n",
       "br           96\n",
       "coconut      83\n",
       "product      63\n",
       "hair         47\n",
       "like         35\n",
       "use          34\n",
       "skin         30\n",
       "just         27\n",
       "saturated    24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poor_review_vec = get_vec(poor_review, 'english')\n",
    "poor_review_vec.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e6269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def lem(sentence):\n",
    "    '''Intake a list of review, lemmatize them and return a new list'''\n",
    "    result_lem = []\n",
    "    for s in tqdm(sentence):\n",
    "        s_lem = lemmatize_sentence(s)\n",
    "        result_lem.append(s_lem)\n",
    "    return result_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4eb1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.append('br')\n",
    "\n",
    "def get_vec(words, stop=stopword_list):\n",
    "    '''Revise the function by changing stop_words and adding token_pattern'''\n",
    "    vectorizer = CountVectorizer(stop_words=stop, lowercase=True, min_df=0.001, token_pattern=r'[a-zA-Z\\_]{3,}', binary=True)\n",
    "    X = vectorizer.fit_transform(words) \n",
    "    X = X.toarray()\n",
    "    print(X.shape)\n",
    "    feature = vectorizer.get_feature_names()\n",
    "    corpus_df = pd.DataFrame(X, columns=feature)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad70b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_replace_post_lem(line):\n",
    "    '''After lemmitization, do the regex cleaning'''\n",
    "    line = re.sub(r'\\b((extra virgin )?coconut oil)\\b', '_coconut_oil_', line)\n",
    "    line = re.sub(r'\\b(olive oil)\\b', '_olive_oil_', line)\n",
    "    line = re.sub(r'\\b(buy|purchase|order)\\b', '_buy_', line)\n",
    "    line = re.sub(r'\\b(jar|container)\\b', '_container_', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bdb3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_lem(line):\n",
    "    line = lem(line)\n",
    "    line = [word_replace_post_lem(review.lower()) for review in line]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67f3b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1124/1124 [00:04<00:00, 231.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124, 2488)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "use              779\n",
       "_coconut_oil_    714\n",
       "great            565\n",
       "product          558\n",
       "love             433\n",
       "oil              431\n",
       "good             406\n",
       "hair             405\n",
       "_buy_            403\n",
       "skin             390\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_review_lem = regex_lem(good_review)\n",
    "good_review_vec = get_vec(good_review_lem)\n",
    "good_review_vec.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4550526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 66/66 [00:00<00:00, 269.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 1063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "product          42\n",
       "use              39\n",
       "_coconut_oil_    34\n",
       "_buy_            33\n",
       "like             28\n",
       "oil              25\n",
       "good             22\n",
       "_container_      21\n",
       "get              20\n",
       "would            18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poor_review_lem = regex_lem(poor_review)\n",
    "poor_review_vec = get_vec(poor_review_lem)\n",
    "poor_review_vec.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2e01a",
   "metadata": {},
   "source": [
    "To better understand the relationship and interpret the result of word count, I choose 2 as the n for n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1be42de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vec(review):\n",
    "    '''Intake a list of review and return tf-idf report'''\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             token_pattern=r'\\b[a-zA-Z\\_]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopword_list, max_features=1000, binary=True)\n",
    "    X = vectorizer.fit_transform(review)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "    tf_idf = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c780c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great product</th>\n",
       "      <td>27.158786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use _coconut_oil_</th>\n",
       "      <td>23.425570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use hair</th>\n",
       "      <td>22.490846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also use</th>\n",
       "      <td>19.986221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organic _coconut_oil_</th>\n",
       "      <td>19.149359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_coconut_oil_ use</th>\n",
       "      <td>18.323303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use cook</th>\n",
       "      <td>17.336455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love _coconut_oil_</th>\n",
       "      <td>16.797652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut flavor</th>\n",
       "      <td>16.215402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love product</th>\n",
       "      <td>16.160967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "great product          27.158786\n",
       "use _coconut_oil_      23.425570\n",
       "use hair               22.490846\n",
       "also use               19.986221\n",
       "organic _coconut_oil_  19.149359\n",
       "_coconut_oil_ use      18.323303\n",
       "use cook               17.336455\n",
       "love _coconut_oil_     16.797652\n",
       "coconut flavor         16.215402\n",
       "love product           16.160967"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_good = get_tfidf_vec(good_review_lem)\n",
    "score_good[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8275e560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plastic _container_</th>\n",
       "      <td>1.592960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_buy_ product</th>\n",
       "      <td>1.371997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read review</th>\n",
       "      <td>0.954246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_coconut_oil_ product</th>\n",
       "      <td>0.897701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair skin</th>\n",
       "      <td>0.891208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use _coconut_oil_</th>\n",
       "      <td>0.843340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use cook</th>\n",
       "      <td>0.833990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product good</th>\n",
       "      <td>0.810991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one tub</th>\n",
       "      <td>0.808410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would _buy_</th>\n",
       "      <td>0.807630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          score\n",
       "plastic _container_    1.592960\n",
       "_buy_ product          1.371997\n",
       "read review            0.954246\n",
       "_coconut_oil_ product  0.897701\n",
       "hair skin              0.891208\n",
       "use _coconut_oil_      0.843340\n",
       "use cook               0.833990\n",
       "product good           0.810991\n",
       "one tub                0.808410\n",
       "would _buy_            0.807630"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_poor = get_tfidf_vec(poor_review_lem)\n",
    "score_poor[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d97eb5",
   "metadata": {},
   "source": [
    "Among good reviews, people frequently mention how they use the product. People usually use the product for their hair, skin and also cooking. \"Flavor\" and \"taste\" are also frequently mentioned in good reviews. In contrast, we can see that customers often complain about plastic containers. To be more specific, we also observe \"oil leak\" appears in top keywords in poor reviews. The manufacturer should revise thier design of containers and find out the reason that lead to leakage. Next, we can see that \"read review\" is in top keywords for poor review. After reading comments, we find that people often read the good review and give it a try but eventually find out the product doesn't meet their needs. We suggest the vendor can make a more detailed description in product introduction to let customers know what they should expect from the product, suitable type of hair & skin, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89287160",
   "metadata": {},
   "source": [
    "Overall, I think TF-IDF make sense since it can capture the correct keywords. However, it can not distinguish the difference of importance in 'summary' and 'text'. The content in summary should be more important in practice. In addition, I think we should also compare TF-IDF of good reviews and bad reviews so that we can capture keywords more precisely. Currently they share several keywords so that it's hard to tell what keywords are actually significant for good and poor reviews respectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63c302",
   "metadata": {},
   "source": [
    "## Part 2: Similarity and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13c9dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.drop_duplicates(['Summary', 'Text'], inplace=True)\n",
    "df['text_all'] = df['Summary'] + ' ' + df['Text']\n",
    "review_all = df['text_all'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a438b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4938/4938 [00:13<00:00, 363.89it/s]\n"
     ]
    }
   ],
   "source": [
    "review_all_lem = lem(review_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "038fdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(review):\n",
    "    '''Intake a list of review and return word count dataframe'''\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z\\_]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopword_list, max_features=1000, binary=True)\n",
    "    X = vectorizer.fit_transform(review)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray(), columns=terms)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bdefe7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorb</th>\n",
       "      <th>acid</th>\n",
       "      <th>acne</th>\n",
       "      <th>across</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>wrap</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolute  absolutely  absorb  acid  acne  across  actual  actually  \\\n",
       "0   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "1   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "2   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "3   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "4   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "5   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "6   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.216141   \n",
       "7   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "8   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "9   0.0       0.0         0.0     0.0   0.0   0.0     0.0     0.0  0.000000   \n",
       "\n",
       "   add  ...  wrap  write  wrong  www      year  yes  yet  young  yum  yummy  \n",
       "0  0.0  ...   0.0    0.0    0.0  0.0  0.154158  0.0  0.0    0.0  0.0    0.0  \n",
       "1  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "2  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "3  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "4  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "5  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "6  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "7  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "8  0.0  ...   0.0    0.0    0.0  0.0  0.162590  0.0  0.0    0.0  0.0    0.0  \n",
       "9  0.0  ...   0.0    0.0    0.0  0.0  0.000000  0.0  0.0    0.0  0.0    0.0  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_all_vec = tfidf(review_all_lem)\n",
    "review_all_vec.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3afbda",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5940f5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4928</th>\n",
       "      <th>4929</th>\n",
       "      <th>4930</th>\n",
       "      <th>4931</th>\n",
       "      <th>4932</th>\n",
       "      <th>4933</th>\n",
       "      <th>4934</th>\n",
       "      <th>4935</th>\n",
       "      <th>4936</th>\n",
       "      <th>4937</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.028843</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.069288</td>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.105333</td>\n",
       "      <td>0.046702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020708</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.042291</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.165724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.234870</td>\n",
       "      <td>0.113478</td>\n",
       "      <td>0.075278</td>\n",
       "      <td>0.065386</td>\n",
       "      <td>0.090242</td>\n",
       "      <td>0.122397</td>\n",
       "      <td>0.055267</td>\n",
       "      <td>0.054684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.079465</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.140169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.234870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399004</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.045639</td>\n",
       "      <td>0.191024</td>\n",
       "      <td>0.039848</td>\n",
       "      <td>0.090243</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>0.041316</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050291</td>\n",
       "      <td>0.021654</td>\n",
       "      <td>0.021185</td>\n",
       "      <td>0.040691</td>\n",
       "      <td>0.046033</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028843</td>\n",
       "      <td>0.113478</td>\n",
       "      <td>0.399004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.137659</td>\n",
       "      <td>0.071574</td>\n",
       "      <td>0.168491</td>\n",
       "      <td>0.024777</td>\n",
       "      <td>0.101064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049970</td>\n",
       "      <td>0.023735</td>\n",
       "      <td>0.054040</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>0.024251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021444</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.075278</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.137659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.073732</td>\n",
       "      <td>0.105518</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.065386</td>\n",
       "      <td>0.045639</td>\n",
       "      <td>0.071574</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210429</td>\n",
       "      <td>0.102702</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>0.054789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038405</td>\n",
       "      <td>0.062456</td>\n",
       "      <td>0.031926</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.069288</td>\n",
       "      <td>0.090242</td>\n",
       "      <td>0.191024</td>\n",
       "      <td>0.168491</td>\n",
       "      <td>0.073732</td>\n",
       "      <td>0.210429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216015</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028750</td>\n",
       "      <td>0.102446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.069412</td>\n",
       "      <td>0.059117</td>\n",
       "      <td>0.056481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.122397</td>\n",
       "      <td>0.039848</td>\n",
       "      <td>0.024777</td>\n",
       "      <td>0.105518</td>\n",
       "      <td>0.102702</td>\n",
       "      <td>0.216015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.180838</td>\n",
       "      <td>0.097907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027450</td>\n",
       "      <td>0.065668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.049721</td>\n",
       "      <td>0.057788</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.044358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.105333</td>\n",
       "      <td>0.055267</td>\n",
       "      <td>0.090243</td>\n",
       "      <td>0.101064</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>0.180838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.054130</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.087247</td>\n",
       "      <td>0.077311</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>0.061809</td>\n",
       "      <td>0.062742</td>\n",
       "      <td>0.032419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.046702</td>\n",
       "      <td>0.054684</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054789</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>0.097907</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006808</td>\n",
       "      <td>0.021541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.011729</td>\n",
       "      <td>0.070480</td>\n",
       "      <td>0.009940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.015773  0.025754  0.028843  0.022723  0.074381  0.069288   \n",
       "1  0.015773  1.000000  0.234870  0.113478  0.075278  0.065386  0.090242   \n",
       "2  0.025754  0.234870  1.000000  0.399004  0.122919  0.045639  0.191024   \n",
       "3  0.028843  0.113478  0.399004  1.000000  0.137659  0.071574  0.168491   \n",
       "4  0.022723  0.075278  0.122919  0.137659  1.000000  0.056569  0.073732   \n",
       "5  0.074381  0.065386  0.045639  0.071574  0.056569  1.000000  0.210429   \n",
       "6  0.069288  0.090242  0.191024  0.168491  0.073732  0.210429  1.000000   \n",
       "7  0.126992  0.122397  0.039848  0.024777  0.105518  0.102702  0.216015   \n",
       "8  0.105333  0.055267  0.090243  0.101064  0.118657  0.116293  0.242393   \n",
       "9  0.046702  0.054684  0.055305  0.000000  0.000000  0.054789  0.054570   \n",
       "\n",
       "       7         8         9     ...      4928      4929      4930      4931  \\\n",
       "0  0.126992  0.105333  0.046702  ...  0.020708  0.007243  0.025913  0.042291   \n",
       "1  0.122397  0.055267  0.054684  ...  0.010391  0.079465  0.012979  0.000000   \n",
       "2  0.039848  0.090243  0.055305  ...  0.047929  0.041316  0.021193  0.000000   \n",
       "3  0.024777  0.101064  0.000000  ...  0.000000  0.049970  0.023735  0.054040   \n",
       "4  0.105518  0.118657  0.000000  ...  0.000000  0.029893  0.000000  0.000000   \n",
       "5  0.102702  0.116293  0.054789  ...  0.038405  0.062456  0.031926  0.023629   \n",
       "6  0.216015  0.242393  0.054570  ...  0.028750  0.102446  0.000000  0.018178   \n",
       "7  1.000000  0.180838  0.097907  ...  0.027450  0.065668  0.000000  0.019918   \n",
       "8  0.180838  1.000000  0.096136  ...  0.037499  0.042253  0.054130  0.016997   \n",
       "9  0.097907  0.096136  1.000000  ...  0.006808  0.021541  0.000000  0.037896   \n",
       "\n",
       "       4932      4933      4934      4935      4936      4937  \n",
       "0  0.072428  0.000000  0.012497  0.000000  0.013220  0.165724  \n",
       "1  0.030799  0.140169  0.000000  0.080257  0.013266  0.000000  \n",
       "2  0.050291  0.021654  0.021185  0.040691  0.046033  0.000000  \n",
       "3  0.022292  0.024251  0.000000  0.021444  0.024259  0.000000  \n",
       "4  0.000000  0.078405  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.052150  0.000000  0.000000  0.000000  0.095588  \n",
       "6  0.018226  0.000000  0.034615  0.069412  0.059117  0.056481  \n",
       "7  0.049721  0.057788  0.010735  0.000000  0.032956  0.044358  \n",
       "8  0.087247  0.077311  0.014809  0.061809  0.062742  0.032419  \n",
       "9  0.000000  0.000000  0.011534  0.011729  0.070480  0.009940  \n",
       "\n",
       "[10 rows x 4938 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = cosine_similarity(review_all_vec)\n",
    "distances_df = pd.DataFrame(distances)\n",
    "distances_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f41d51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table = distances_df.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()\n",
    "similarity_table.columns = [\"review1\", \"review2\", \"similarity\"]\n",
    "def sort_indices(index1, index2)-> str:\n",
    "    indices = [str(index1), str(index2)]\n",
    "    indices.sort()\n",
    "    return \"\".join(indices)\n",
    "\n",
    "similarity_table[\"index\"] = similarity_table.apply(lambda x: sort_indices(x.review1, x.review2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4975bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table.drop_duplicates(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "589ca34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review1</th>\n",
       "      <th>review2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17207674</th>\n",
       "      <td>3484</td>\n",
       "      <td>3682</td>\n",
       "      <td>0.992418</td>\n",
       "      <td>3484.03682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612773</th>\n",
       "      <td>529</td>\n",
       "      <td>571</td>\n",
       "      <td>0.991852</td>\n",
       "      <td>529.0571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8949482</th>\n",
       "      <td>1812</td>\n",
       "      <td>1826</td>\n",
       "      <td>0.980817</td>\n",
       "      <td>1812.01826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19153443</th>\n",
       "      <td>3878</td>\n",
       "      <td>3879</td>\n",
       "      <td>0.975672</td>\n",
       "      <td>3878.03879.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21247579</th>\n",
       "      <td>4302</td>\n",
       "      <td>4303</td>\n",
       "      <td>0.972717</td>\n",
       "      <td>4302.04303.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review1  review2  similarity         index\n",
       "17207674     3484     3682    0.992418  3484.03682.0\n",
       "2612773       529      571    0.991852    529.0571.0\n",
       "8949482      1812     1826    0.980817  1812.01826.0\n",
       "19153443     3878     3879    0.975672  3878.03879.0\n",
       "21247579     4302     4303    0.972717  4302.04303.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_most_similar = similarity_table[similarity_table.similarity < 0.999].sort_values(by=\"similarity\", ascending=False).head(5)\n",
    "top_5_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f86511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: Great Taste Grove Square Cappuccino Cups be excellent . Tasted really good right from the Keurig brewer with nothing add . wWould highly recommend . RCCJR\n",
      "Review 2: Excellent taste Grove Square Cappuccino Cups be excellent . Tasted really good right from the Keurig brewer with nothing add . wWould highly recommend . RCCJR\n"
     ]
    }
   ],
   "source": [
    "print(\"Review 1:\", review_all_lem[3484])\n",
    "print(\"Review 2:\", review_all_lem[3682])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452aeb1",
   "metadata": {},
   "source": [
    "### Euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5782f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4928</th>\n",
       "      <th>4929</th>\n",
       "      <th>4930</th>\n",
       "      <th>4931</th>\n",
       "      <th>4932</th>\n",
       "      <th>4933</th>\n",
       "      <th>4934</th>\n",
       "      <th>4935</th>\n",
       "      <th>4936</th>\n",
       "      <th>4937</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.403016</td>\n",
       "      <td>1.395884</td>\n",
       "      <td>1.393670</td>\n",
       "      <td>1.398054</td>\n",
       "      <td>1.360602</td>\n",
       "      <td>1.364340</td>\n",
       "      <td>1.321369</td>\n",
       "      <td>1.337660</td>\n",
       "      <td>1.380795</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399494</td>\n",
       "      <td>1.409083</td>\n",
       "      <td>1.395770</td>\n",
       "      <td>1.383987</td>\n",
       "      <td>1.362037</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.405349</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.404834</td>\n",
       "      <td>1.291725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.403016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.237036</td>\n",
       "      <td>1.331557</td>\n",
       "      <td>1.359942</td>\n",
       "      <td>1.367197</td>\n",
       "      <td>1.348894</td>\n",
       "      <td>1.324842</td>\n",
       "      <td>1.374579</td>\n",
       "      <td>1.375002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.406847</td>\n",
       "      <td>1.356860</td>\n",
       "      <td>1.405006</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.392265</td>\n",
       "      <td>1.311359</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.356276</td>\n",
       "      <td>1.404802</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.395884</td>\n",
       "      <td>1.237036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.096354</td>\n",
       "      <td>1.324447</td>\n",
       "      <td>1.381565</td>\n",
       "      <td>1.271988</td>\n",
       "      <td>1.385750</td>\n",
       "      <td>1.348894</td>\n",
       "      <td>1.374551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379907</td>\n",
       "      <td>1.384690</td>\n",
       "      <td>1.399147</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.378194</td>\n",
       "      <td>1.398818</td>\n",
       "      <td>1.399153</td>\n",
       "      <td>1.385142</td>\n",
       "      <td>1.381280</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.393670</td>\n",
       "      <td>1.331557</td>\n",
       "      <td>1.096354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.313272</td>\n",
       "      <td>1.362664</td>\n",
       "      <td>1.289580</td>\n",
       "      <td>1.396583</td>\n",
       "      <td>1.340848</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>...</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.378427</td>\n",
       "      <td>1.397330</td>\n",
       "      <td>1.375471</td>\n",
       "      <td>1.398362</td>\n",
       "      <td>1.396960</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.398968</td>\n",
       "      <td>1.396954</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.398054</td>\n",
       "      <td>1.359942</td>\n",
       "      <td>1.324447</td>\n",
       "      <td>1.313272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.373631</td>\n",
       "      <td>1.361079</td>\n",
       "      <td>1.337521</td>\n",
       "      <td>1.327662</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>...</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.392916</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.357641</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.360602</td>\n",
       "      <td>1.367197</td>\n",
       "      <td>1.381565</td>\n",
       "      <td>1.362664</td>\n",
       "      <td>1.373631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.256639</td>\n",
       "      <td>1.339625</td>\n",
       "      <td>1.329441</td>\n",
       "      <td>1.374926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386792</td>\n",
       "      <td>1.369339</td>\n",
       "      <td>1.391456</td>\n",
       "      <td>1.397406</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.376844</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.344926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.364340</td>\n",
       "      <td>1.348894</td>\n",
       "      <td>1.271988</td>\n",
       "      <td>1.289580</td>\n",
       "      <td>1.361079</td>\n",
       "      <td>1.256639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.252186</td>\n",
       "      <td>1.230941</td>\n",
       "      <td>1.375086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393736</td>\n",
       "      <td>1.339816</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.401301</td>\n",
       "      <td>1.401266</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.389521</td>\n",
       "      <td>1.364249</td>\n",
       "      <td>1.371775</td>\n",
       "      <td>1.373695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.321369</td>\n",
       "      <td>1.324842</td>\n",
       "      <td>1.385750</td>\n",
       "      <td>1.396583</td>\n",
       "      <td>1.337521</td>\n",
       "      <td>1.339625</td>\n",
       "      <td>1.252186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.279970</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.394669</td>\n",
       "      <td>1.366991</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.400059</td>\n",
       "      <td>1.378607</td>\n",
       "      <td>1.372743</td>\n",
       "      <td>1.406602</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.390715</td>\n",
       "      <td>1.382492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.337660</td>\n",
       "      <td>1.374579</td>\n",
       "      <td>1.348894</td>\n",
       "      <td>1.340848</td>\n",
       "      <td>1.327662</td>\n",
       "      <td>1.329441</td>\n",
       "      <td>1.230941</td>\n",
       "      <td>1.279970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.344518</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387444</td>\n",
       "      <td>1.384014</td>\n",
       "      <td>1.375405</td>\n",
       "      <td>1.402143</td>\n",
       "      <td>1.351113</td>\n",
       "      <td>1.358447</td>\n",
       "      <td>1.403703</td>\n",
       "      <td>1.369811</td>\n",
       "      <td>1.369129</td>\n",
       "      <td>1.391101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.380795</td>\n",
       "      <td>1.375002</td>\n",
       "      <td>1.374551</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.374926</td>\n",
       "      <td>1.375086</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.344518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.409391</td>\n",
       "      <td>1.398899</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.387158</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.406034</td>\n",
       "      <td>1.405896</td>\n",
       "      <td>1.363466</td>\n",
       "      <td>1.407167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.000000  1.403016  1.395884  1.393670  1.398054  1.360602  1.364340   \n",
       "1  1.403016  0.000000  1.237036  1.331557  1.359942  1.367197  1.348894   \n",
       "2  1.395884  1.237036  0.000000  1.096354  1.324447  1.381565  1.271988   \n",
       "3  1.393670  1.331557  1.096354  0.000000  1.313272  1.362664  1.289580   \n",
       "4  1.398054  1.359942  1.324447  1.313272  0.000000  1.373631  1.361079   \n",
       "5  1.360602  1.367197  1.381565  1.362664  1.373631  0.000000  1.256639   \n",
       "6  1.364340  1.348894  1.271988  1.289580  1.361079  1.256639  0.000000   \n",
       "7  1.321369  1.324842  1.385750  1.396583  1.337521  1.339625  1.252186   \n",
       "8  1.337660  1.374579  1.348894  1.340848  1.327662  1.329441  1.230941   \n",
       "9  1.380795  1.375002  1.374551  1.414214  1.414214  1.374926  1.375086   \n",
       "\n",
       "       7         8         9     ...      4928      4929      4930      4931  \\\n",
       "0  1.321369  1.337660  1.380795  ...  1.399494  1.409083  1.395770  1.383987   \n",
       "1  1.324842  1.374579  1.375002  ...  1.406847  1.356860  1.405006  1.414214   \n",
       "2  1.385750  1.348894  1.374551  ...  1.379907  1.384690  1.399147  1.414214   \n",
       "3  1.396583  1.340848  1.414214  ...  1.414214  1.378427  1.397330  1.375471   \n",
       "4  1.337521  1.327662  1.414214  ...  1.414214  1.392916  1.414214  1.414214   \n",
       "5  1.339625  1.329441  1.374926  ...  1.386792  1.369339  1.391456  1.397406   \n",
       "6  1.252186  1.230941  1.375086  ...  1.393736  1.339816  1.414214  1.401301   \n",
       "7  0.000000  1.279970  1.343200  ...  1.394669  1.366991  1.414214  1.400059   \n",
       "8  1.279970  0.000000  1.344518  ...  1.387444  1.384014  1.375405  1.402143   \n",
       "9  1.343200  1.344518  0.000000  ...  1.409391  1.398899  1.414214  1.387158   \n",
       "\n",
       "       4932      4933      4934      4935      4936      4937  \n",
       "0  1.362037  1.414214  1.405349  1.414214  1.404834  1.291725  \n",
       "1  1.392265  1.311359  1.414214  1.356276  1.404802  1.414214  \n",
       "2  1.378194  1.398818  1.399153  1.385142  1.381280  1.414214  \n",
       "3  1.398362  1.396960  1.414214  1.398968  1.396954  1.414214  \n",
       "4  1.414214  1.357641  1.414214  1.414214  1.414214  1.414214  \n",
       "5  1.414214  1.376844  1.414214  1.414214  1.414214  1.344926  \n",
       "6  1.401266  1.414214  1.389521  1.364249  1.371775  1.373695  \n",
       "7  1.378607  1.372743  1.406602  1.414214  1.390715  1.382492  \n",
       "8  1.351113  1.358447  1.403703  1.369811  1.369129  1.391101  \n",
       "9  1.414214  1.414214  1.406034  1.405896  1.363466  1.407167  \n",
       "\n",
       "[10 rows x 4938 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_e = euclidean_distances(review_all_vec)\n",
    "distances_e_df = pd.DataFrame(distances_e)\n",
    "distances_e_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baeb7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_e_table = distances_e_df.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()\n",
    "similarity_e_table.columns = [\"review1\", \"review2\", \"similarity\"]\n",
    "similarity_e_table[\"index\"] = similarity_e_table.apply(lambda x: sort_indices(x.review1, x.review2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f58dda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_e_table.drop_duplicates(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8437b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review1</th>\n",
       "      <th>review2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7255393</th>\n",
       "      <td>1469</td>\n",
       "      <td>1471</td>\n",
       "      <td>2.107342e-08</td>\n",
       "      <td>1469.01471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7255392</th>\n",
       "      <td>1469</td>\n",
       "      <td>1470</td>\n",
       "      <td>2.107342e-08</td>\n",
       "      <td>1469.01470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7260331</th>\n",
       "      <td>1470</td>\n",
       "      <td>1471</td>\n",
       "      <td>2.107342e-08</td>\n",
       "      <td>1470.01471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17207674</th>\n",
       "      <td>3484</td>\n",
       "      <td>3682</td>\n",
       "      <td>1.231460e-01</td>\n",
       "      <td>3484.03682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612773</th>\n",
       "      <td>529</td>\n",
       "      <td>571</td>\n",
       "      <td>1.276537e-01</td>\n",
       "      <td>529.0571.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review1  review2    similarity         index\n",
       "7255393      1469     1471  2.107342e-08  1469.01471.0\n",
       "7255392      1469     1470  2.107342e-08  1469.01470.0\n",
       "7260331      1470     1471  2.107342e-08  1470.01471.0\n",
       "17207674     3484     3682  1.231460e-01  3484.03682.0\n",
       "2612773       529      571  1.276537e-01    529.0571.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_most_similar_e = similarity_e_table[similarity_e_table.similarity > 0].sort_values(by=\"similarity\", ascending=True).head(5)\n",
    "top_5_most_similar_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11442b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: This picky eater love them ! I be a little hesitant to try these , especially after read such mixed review although overall they be positive . However , I like the idea that they be healthy than regular chip and I figure they would be great for school lunch , quick snack , etc . I decide to give them a try and figured if we do not like them , they could be donate . I order a mixed case the first time around to see which flavor we like more ( or less ) and think that would give us a good sampling . After decide we like most of the flavor , we decide to try other one as well. < br / > < br / > Despite the flavor preference , these chip do not taste like cardboard . At first taste , they be a little crunchier and thicker than you might expect . I think this be a good thing , because it mean they be not heavily-laden with oil . The 0.8 oz bag be 100 calorie each as well , which be much good than the other alternative and give you the enjoyment of chip without the extra calorie , trans fat , etc. < br / > < br / > I have include a summary of opinion below from myself , friend and family for each flavor : < br / > < br / > Sea Salt & Vinegar - Great , the absolute favorite flavor for everyone < br / > BBQ - Very good , I 'm not crazy about BBQ to begin with but would say this flavor be `` sweet '' BBQ . Everyone else love them. < br / > Cheddar - Very Good , probably close in preference to the BBQ < br / > Original Potato - Good , but a little salty ( seem to fluctuate somewhat between bag ) < br / > Salt & Pepper - Good , everyone like them but prefer other flavor more < br / > Parmesan Garlic - Good , but overall be 50/50 . You definitely taste more parmesan than garlic and most people be expect the latter. < br / > Sour Cream & Onion - Not so good , no one really liked these enough to want more or to even finish the bag < br / > < br / > I know taste be a very subjective thing , but I hope this review help someone decide to give these a try . I do get them at a reduced price through subscribe & save , and shortly after my first order , Amazon have them on the Gold Box for an even good price . I think the average price per bag come out to approximately forty six cent a bag and have free shipping under my prime account .\n",
      "Review 2: From a picky eater - I love these I be a little hesitant to try these , especially after read such mixed review although overall they be positive . However , I like the idea that they be healthy than regular chip and I figure they would be great for school lunch , quick snack , etc . I decide to give them a try and figured if we do not like them , they could be donate . I order a mixed case the first time around to see which flavor we like more ( or less ) and think that would give us a good sampling . After decide we like most of the flavor , we decide to try other one as well. < br / > < br / > Despite the flavor preference , these chip do not taste like cardboard . At first taste , they be a little crunchier and thicker than you might expect . I think this be a good thing , because it mean they be not heavily-laden with oil . The 0.8 oz bag be 100 calorie each as well , which be much good than the other alternative and give you the enjoyment of chip without the extra calorie , trans fat , etc. < br / > < br / > I have include a summary of opinion below from myself , friend and family for each flavor : < br / > < br / > Sea Salt & Vinegar - Great , the absolute favorite flavor for everyone < br / > BBQ - Very good , I 'm not crazy about BBQ to begin with but would say this flavor be `` sweet '' BBQ . Everyone else love them. < br / > Cheddar - Very Good , probably close in preference to the BBQ < br / > Original Potato - Good , but a little salty ( seem to fluctuate somewhat between bag ) < br / > Salt & Pepper - Good , everyone like them but prefer other flavor more < br / > Parmesan Garlic - Good , but overall be 50/50 . You definitely taste more parmesan than garlic and most people be expect the latter. < br / > Sour Cream & Onion - Not so good , no one really liked these enough to want more or to even finish the bag < br / > < br / > I know taste be a very subjective thing , but I hope this review help someone decide to give these a try . I do get them at a reduced price through subscribe & save , and shortly after my first order , Amazon have them on the Gold Box for an even good price . I think the average price per bag come out to approximately forty six cent a bag and have free shipping under my prime account .\n"
     ]
    }
   ],
   "source": [
    "print(\"Review 1:\", review_all_lem[1469])\n",
    "print(\"Review 2:\", review_all_lem[1471])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe803263",
   "metadata": {},
   "source": [
    "## Part 3: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4bae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Love this movie. Can’t wait!\", \"Yes\"],\n",
    "    [\"I want to see this movie so bad.\", \"Yes\"],\n",
    "    [\"This movie looks amazing.\", \"Yes\"],\n",
    "    [\"Looks bad.\", \"No\"],\n",
    "    [\"Hard pass to see this bad movie.\", \"No\"],\n",
    "    [\"So boring!\", \"No\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c046eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    document[0] = document[0].lower().replace('.', '').replace('!', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5f61c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "stop = ['to', 'this']\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        if word not in stop:\n",
    "            corpus.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa252188",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_yes\", \"likelihood_given_no\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef293522",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "523a7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "yes_documents = 0\n",
    "no_documents = 0\n",
    "for doc, label in documents:\n",
    "    if label == \"Yes\":\n",
    "        yes_documents += 1\n",
    "    else:\n",
    "        no_documents += 1\n",
    "    \n",
    "p_yes = yes_documents / (no_documents + yes_documents)\n",
    "p_no = no_documents / (no_documents + yes_documents)\n",
    "print(p_yes, p_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ec12a",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90566d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likelihood_given_yes</th>\n",
       "      <th>likelihood_given_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can’t</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        likelihood_given_yes likelihood_given_no\n",
       "hard                     0.0            0.333333\n",
       "movie                    1.0            0.333333\n",
       "amazing             0.333333                 0.0\n",
       "wait                0.333333                 0.0\n",
       "i                   0.333333                 0.0\n",
       "want                0.333333                 0.0\n",
       "see                 0.333333            0.333333\n",
       "boring                   0.0            0.333333\n",
       "can’t               0.333333                 0.0\n",
       "so                  0.333333            0.333333"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    yes_documents_with_word = 0\n",
    "    no_documents_with_word = 0\n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"Yes\":\n",
    "                yes_documents_with_word += 1\n",
    "            else:\n",
    "                no_documents_with_word += 1\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_yes\"] = yes_documents_with_word * 1.0 / yes_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_no\"] = no_documents_with_word * 1.0 / no_documents\n",
    "conditional_probabilities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07b85a",
   "metadata": {},
   "source": [
    "### Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "314c3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"This looks so bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c13804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "def get_likelihood(test_document: str, conditional_probabilities: Dict, stop)-> Tuple[float, float]:\n",
    "    test_document = test_document.lower().replace('.', '').replace('!', '')\n",
    "    likelihood_yes = 1\n",
    "    likelihood_no = 1\n",
    "    for word in test_document.split():\n",
    "        if word not in stop:\n",
    "            likelihood_yes = likelihood_yes * conditional_probabilities.loc[word, \"likelihood_given_yes\"]\n",
    "            likelihood_no = likelihood_no * conditional_probabilities.loc[word, \"likelihood_given_no\"]\n",
    "    return likelihood_yes, likelihood_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4e94edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_yes, likelihood_no = get_likelihood(test_document, conditional_probabilities, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c08542f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037037037037037035 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "print(likelihood_yes, likelihood_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce8ac637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_yes: float, likelihood_no: float, p_yes: float, p_no: float)-> float:\n",
    "    posterior_yes = likelihood_yes * p_yes / (likelihood_yes * p_yes + likelihood_no * p_no)\n",
    "    posterior_no = likelihood_no * p_no / (likelihood_yes * p_yes + likelihood_no * p_no)\n",
    "    return posterior_yes, posterior_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb328733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.6666666666666666)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posterior(likelihood_yes, likelihood_no, p_yes, p_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa9038",
   "metadata": {},
   "source": [
    "To sum up, this sentence is more likely a \"not intent to buy\" review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd02205790e15d2e0e7826f552575388932e4ce5a7ab9e16340e28300e3e5bd9db7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
